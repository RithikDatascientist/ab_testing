{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7681ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.7.0.dev20250310+cu124\n",
      "GPU Available: True\n",
      "GPU Name: NVIDIA GeForce RTX 5060\n",
      "âœ… Matrix multiplication on GPU successful!\n",
      "Result shape: torch.Size([1000, 1000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RITHIK KUMAR\\.conda\\envs\\torch_env\\Lib\\site-packages\\torch\\cuda\\__init__.py:287: UserWarning: \n",
      "NVIDIA GeForce RTX 5060 with CUDA capability sm_120 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_50 sm_60 sm_61 sm_70 sm_75 sm_80 sm_86 sm_90.\n",
      "If you want to use the NVIDIA GeForce RTX 5060 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check GPU\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Simple GPU test\n",
    "x = torch.randn(1000, 1000).cuda()\n",
    "y = x @ x\n",
    "print(f\"âœ… Matrix multiplication on GPU successful!\")\n",
    "print(f\"Result shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34387a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Time: 3.16s\n",
      "CPU Time: 22.67s\n",
      "Speedup: 7.2x faster on GPU! ðŸš€\n"
     ]
    }
   ],
   "source": [
    "# Test GPU performance\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# GPU test\n",
    "x = torch.randn(5000, 5000).cuda()\n",
    "start = time.time()\n",
    "for _ in range(50):\n",
    "    y = x @ x\n",
    "torch.cuda.synchronize()\n",
    "gpu_time = time.time() - start\n",
    "\n",
    "# CPU test\n",
    "x_cpu = torch.randn(5000, 5000)\n",
    "start = time.time()\n",
    "for _ in range(50):\n",
    "    y_cpu = x_cpu @ x_cpu\n",
    "cpu_time = time.time() - start\n",
    "\n",
    "print(f\"GPU Time: {gpu_time:.2f}s\")\n",
    "print(f\"CPU Time: {cpu_time:.2f}s\")\n",
    "print(f\"Speedup: {cpu_time/gpu_time:.1f}x faster on GPU! ðŸš€\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81b660fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total VRAM: 7.96 GB\n",
      "Allocated: 0.20 GB\n",
      "Available: 7.76 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check your available VRAM\n",
    "print(f\"Total VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "print(f\"Available: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()) / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780b9d77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
